{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bayesian neural network using mean-field variational inference.\n",
    "(see, e.g., Blundell et al. (2015); Kucukelbir et al. (2016))\n",
    "Inspired by autograd's Bayesian neural network example.\n",
    "\n",
    "Probability model:\n",
    "  Bayesian neural network\n",
    "  Prior: Normal\n",
    "  Likelihood: Normal with mean parameterized by fully connected NN\n",
    "Variational model\n",
    "  Likelihood: Mean-field Normal\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Variational, Normal\n",
    "from edward.stats import norm\n",
    "\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "class BayesianNN:\n",
    "  \"\"\"\n",
    "  Bayesian neural network for regressing outputs y on inputs x.\n",
    "\n",
    "  p((x,y), z) = Normal(y | NN(x; z), lik_variance) *\n",
    "          Normal(z | 0, 1),\n",
    "\n",
    "  where z are neural network weights, and with known lik_variance.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  layer_sizes : list\n",
    "    The size of each layer, ordered from input to output.\n",
    "  nonlinearity : function, optional\n",
    "    Non-linearity after each linear transformation in the neural\n",
    "    network; aka activation function.\n",
    "  lik_variance : float, optional\n",
    "    Variance of the normal likelihood; aka noise parameter,\n",
    "    homoscedastic variance, scale parameter.\n",
    "  \"\"\"\n",
    "  def __init__(self, layer_sizes, nonlinearity=tf.nn.tanh,\n",
    "               lik_variance=0.01):\n",
    "    self.layer_sizes = layer_sizes\n",
    "    self.nonlinearity = nonlinearity\n",
    "    self.lik_variance = lik_variance\n",
    "\n",
    "    self.n_layers = len(layer_sizes)\n",
    "    self.weight_dims = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
    "    self.n_vars = sum((m + 1) * n for m, n in self.weight_dims)\n",
    "\n",
    "  def unpack_weights(self, z):\n",
    "    \"\"\"Unpack weight matrices and biases from a flattened vector.\"\"\"\n",
    "    for m, n in self.weight_dims:\n",
    "      yield tf.reshape(z[:m * n], [m, n]), \\\n",
    "          tf.reshape(z[m * n:(m * n + n)], [1, n])\n",
    "      z = z[(m + 1) * n:]\n",
    "\n",
    "  def neural_network(self, x, zs):\n",
    "    \"\"\"\n",
    "    Return a `n_samples` x `n_minibatch` matrix. Each row is\n",
    "    the output of a neural network on the input data `x` and\n",
    "    given a set of weights `z` in `zs`.\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    for z in tf.unpack(zs):\n",
    "      # Calculate neural network with weights given by `z`.\n",
    "      h = x\n",
    "      for W, b in self.unpack_weights(z):\n",
    "        # broadcasting to do (h*W) + b (e.g. 40x10 + 1x10)\n",
    "        h = self.nonlinearity(tf.matmul(h, W) + b)\n",
    "\n",
    "      matrix += [tf.squeeze(h)]  # n_minibatch x 1 to n_minibatch\n",
    "\n",
    "    return tf.pack(matrix)\n",
    "\n",
    "  def log_lik(self, xs, zs):\n",
    "    \"\"\"Return a vector [log p(xs | zs[1,:]), ..., log p(xs | zs[S,:])].\"\"\"\n",
    "    x, y = xs['x'], xs['y']\n",
    "    mus = self.neural_network(x, zs)\n",
    "    log_lik = tf.reduce_sum(norm.logpdf(y, loc=mus, scale=self.lik_variance), 1)\n",
    "    return log_lik\n",
    "\n",
    "\n",
    "def build_toy_dataset(N=50, noise_std=0.1):\n",
    "  x = np.linspace(-3, 3, num=N)\n",
    "  y = np.cos(x) + norm.rvs(0, noise_std, size=N)\n",
    "  x = x.reshape((N, 1))\n",
    "  return {'x': x, 'y': y}\n",
    "\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "model = BayesianNN(layer_sizes=[1, 2, 2, 1])\n",
    "\n",
    "variational = Variational()\n",
    "variational.add(Normal(model.n_vars))\n",
    "\n",
    "data = build_toy_dataset()\n",
    "\n",
    "inference = ed.MFVI(model, variational, data)\n",
    "inference.initialize()\n",
    "\n",
    "\n",
    "sess = ed.get_session()\n",
    "\n",
    "# FIRST VISUALIZATION (prior)\n",
    "print(\"first visualization -- prior\")\n",
    "\n",
    "# Sample functions from variational model\n",
    "mean, std = sess.run([variational.layers[0].loc,\n",
    "                      variational.layers[0].scale])\n",
    "rs = np.random.RandomState(0)\n",
    "zs = rs.randn(10, variational.n_vars) * std + mean\n",
    "zs = tf.constant(zs, dtype=tf.float32)\n",
    "inputs = np.linspace(-5, 5, num=400, dtype=np.float32)\n",
    "x = tf.expand_dims(tf.constant(inputs), 1)\n",
    "mus = model.neural_network(x, zs)\n",
    "outputs = mus.eval()\n",
    "x, y = data['x'], data['y']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}